{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from model import Net1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx"
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa7ab33-cf28-4ff4-c30f-68676f5896c6"
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 262899472.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 26652517.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 144538776.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3764182.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968d0a1e-f580-40b4-8e3e-68e8cb0b1715"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b89d1b-40a5-4522-e9fd-4ab05cf6b23d"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net1().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 12, 26, 26]             108\n",
            "              ReLU-2           [-1, 12, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 12, 26, 26]              24\n",
            "            Conv2d-4           [-1, 14, 24, 24]           1,512\n",
            "              ReLU-5           [-1, 14, 24, 24]               0\n",
            "       BatchNorm2d-6           [-1, 14, 24, 24]              28\n",
            "            Conv2d-7           [-1, 10, 24, 24]             140\n",
            "         MaxPool2d-8           [-1, 10, 12, 12]               0\n",
            "            Conv2d-9           [-1, 14, 10, 10]           1,260\n",
            "             ReLU-10           [-1, 14, 10, 10]               0\n",
            "      BatchNorm2d-11           [-1, 14, 10, 10]              28\n",
            "           Conv2d-12             [-1, 16, 8, 8]           2,016\n",
            "             ReLU-13             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
            "           Conv2d-15             [-1, 18, 6, 6]           2,592\n",
            "             ReLU-16             [-1, 18, 6, 6]               0\n",
            "      BatchNorm2d-17             [-1, 18, 6, 6]              36\n",
            "        AvgPool2d-18             [-1, 18, 1, 1]               0\n",
            "           Conv2d-19             [-1, 10, 1, 1]             180\n",
            "================================================================\n",
            "Total params: 7,956\n",
            "Trainable params: 7,956\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.50\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs.\n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE5Le6FYHhc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02623cc7-4941-4a70-b786-bdc5d30c701a"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net1().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    # scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10492228716611862 Batch_id=468 Accuracy=89.78: 100%|██████████| 469/469 [00:17<00:00, 27.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1379, Accuracy: 9611/10000 (96.11%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05515153333544731 Batch_id=468 Accuracy=98.05: 100%|██████████| 469/469 [00:17<00:00, 27.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0614, Accuracy: 9841/10000 (98.41%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02919640950858593 Batch_id=468 Accuracy=98.51: 100%|██████████| 469/469 [00:18<00:00, 25.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0456, Accuracy: 9874/10000 (98.74%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.007329074665904045 Batch_id=468 Accuracy=98.78: 100%|██████████| 469/469 [00:17<00:00, 26.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.019504597410559654 Batch_id=468 Accuracy=98.88: 100%|██████████| 469/469 [00:18<00:00, 25.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.016507454216480255 Batch_id=468 Accuracy=99.03: 100%|██████████| 469/469 [00:17<00:00, 26.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 9918/10000 (99.18%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.00689971586689353 Batch_id=468 Accuracy=99.10: 100%|██████████| 469/469 [00:19<00:00, 23.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0382, Accuracy: 9876/10000 (98.76%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.01010820735245943 Batch_id=468 Accuracy=99.16: 100%|██████████| 469/469 [00:18<00:00, 24.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0331, Accuracy: 9892/10000 (98.92%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04577996954321861 Batch_id=468 Accuracy=99.18: 100%|██████████| 469/469 [00:17<00:00, 26.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9904/10000 (99.04%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03311764821410179 Batch_id=468 Accuracy=99.32: 100%|██████████| 469/469 [00:18<00:00, 24.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9909/10000 (99.09%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03359479084610939 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:18<00:00, 25.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0274, Accuracy: 9913/10000 (99.13%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.009198218584060669 Batch_id=468 Accuracy=99.35: 100%|██████████| 469/469 [00:17<00:00, 26.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.014225449413061142 Batch_id=468 Accuracy=99.35: 100%|██████████| 469/469 [00:18<00:00, 25.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0338, Accuracy: 9892/10000 (98.92%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.022729849442839622 Batch_id=468 Accuracy=99.40: 100%|██████████| 469/469 [00:18<00:00, 25.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0262, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03032149188220501 Batch_id=468 Accuracy=99.43: 100%|██████████| 469/469 [00:19<00:00, 24.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0277, Accuracy: 9914/10000 (99.14%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKeBz_bFDIBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}